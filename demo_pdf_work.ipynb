{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ryanrien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryanrien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"test_pdf/dummy_test.pdf\"\n",
    "with open(path, \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)\n",
    "#type(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## page count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over all the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for page in pdf:\n",
    "    text+=page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.6788966015096, '11')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Readability(text)\n",
    "fk = r.flesch_kincaid()\n",
    "fk.score, fk.grade_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Analysis of Race and Gender Bias in Deep Age\n",
      "                   Estimation Models\n",
      "                                          Andraž Puc, Vitomir Štruc, Klemen Grm\n",
      "                                  University of Ljubljana, Faculty of Electrical Engineering\n",
      "                                       Tržaška cesta 25, SI-1000 Ljubljana, Slovenia\n",
      "                                                    klemen.grm@fe.uni-lj.si\n",
      "\n",
      "\n",
      "   Abstract—Due to advances in deep learning and convolutional\n",
      "neural networks (CNNs) there has been significant progress in\n",
      "the field of visual age estimation from face images over recent\n",
      "years. While today’s models are able to achieve considerable age\n",
      "estimation accuracy, their behaviour, especially with respect to\n",
      "specific demographic groups is still not well understood. In this\n",
      "paper, we take a deeper look at CNN-based age estimation models\n",
      "and analyze their performance across different race and gender\n",
      "groups. We use two publicly available off-the-shelf age estimation\n",
      "models, i.e., FaceNet and WideResNet, for our study and analyze        Fig. 1: Age estimation from face images has progressed\n",
      "their performance on the UTKFace and APPA-REAL datasets.               considerably in recent years with state-of-the-art models pro-\n",
      "We partition face images into sub-groups based on race, gender         ducing highly accurate estimation results. In this paper we\n",
      "and combinations of race and gender. We then compare age\n",
      "estimation results and find that there are noticeable differences in\n",
      "                                                                       analyze and compare age estimation performance across dif-\n",
      "performance across demographics. Specifically, our results show        ferent demographic groups in terms of both gender and race.\n",
      "that age estimation accuracy is consistently higher for men than\n",
      "for women, while race does not appear to have consistent effects            for certain races, the age estimation models seem to favor\n",
      "on the tested models across different test datasets.\n",
      "                                                                            men over women in term of estimation errors in general.\n",
      "                      I. I NTRODUCTION                                    • We observe no consistent impact of race on age esti-\n",
      "\n",
      "   Age estimation from facial images (illustrated in Fig. 1)                mation accuracy. While different race groups produce\n",
      "has seen increased interest from the machine learning and                   consistent performance variations with all tested models,\n",
      "computer vision communities recently [3], [19], [22], [29]. The             these appear to be inconsistent between different test\n",
      "possibility of determining age from a face image automatically              datasets, suggesting that other nuisance factors affect\n",
      "and with high accuracy can facilitate applications with con-                results to a greater extent than race.\n",
      "siderable market potential, such as detecting minors for legal            The rest of the paper is structured as follows: In Section II\n",
      "purposes or adjusting application interfaces based on users’           we briefly review existing work related to our study. Next, we\n",
      "age. However, it is paramount to understand the behaviour              elaborate on the methodology used in the paper in Section III\n",
      "of existing age estimation models, especially with respect to          and discuss experimental findings in Section IV. We conclude\n",
      "their performance across different demographic groups, when            the paper with some final comments in Section V.\n",
      "deploying them in real-life applications .\n",
      "   While existing work has looked at the impact of demograph-                               II. R ELATED WORK\n",
      "ics when evaluating new models, e.g., [4], this has mostly been           In this section, we present prior work that relates to our\n",
      "a side result of the overall experimental evaluation. Studies          study. We first discuss existing techniques for age estimation,\n",
      "focusing specifically on demographic model bias, on the other          then review existing studies on the impact of gender and race\n",
      "hand, are still limited in the literature. In this paper, we try       in various face-related tasks and finally elaborate on existing\n",
      "to fill this gap and study the impact of race and gender on            work on bias in age estimation models.\n",
      "the accuracy of contemporary deep age estimation models.                  Age estimation. One of the early attempts at age estimation\n",
      "Specifically, we experiment with two pre-trained off-the-shelf         was presented by Kwon et al. in [17] and used an active\n",
      "age estimation models and evaluate their performance on                contour snakelet model that focused on wrinkles and simplified\n",
      "two publicly available datasets. The main contribution of our          the age estimation task into a binary classification problem.\n",
      "work are important findings that help to better understand             In [18], Lanitis et al. described an automatic age estimation\n",
      "age estimation models and their performance on different sub-          approach relying on Active Appearance Models (AAMs) to\n",
      "groups of subjects, such as:                                           jointly extract shape and texture information from an input\n",
      "   • We report results that suggest that age estimation with           face. Later Geng et al. [11] proposed a new approach by\n",
      "      the tested models is more accurate for male subjects than        modeling aging patterns with representative sub-spaces. Guo\n",
      "      for female subjects. While we observe opposite settings          et al. [14] used bio-inspired features (BIFs) and a multilayer\n",
      "\n",
      "\n",
      " 978-9-0827-9705-3                                                 830                                               EUSIPCO 2020\n",
      "\f",
      "\n",
      "HMAX model for age estimation. Chang et al. [6] proposed             relating the performance of apparent and real-age estimation\n",
      "replacing traditional multi-class labels with new ordinally ar-      tasks. In these paper we contribute to a better understanding\n",
      "ranged labels [10], [30] and developed a cost-sensitive ordinal      of the bias of age estimation models with an analysis of the\n",
      "ranking framework for age estimation. Chao et al. [7] proposed       performance of two recent deep learning models with respect\n",
      "an age-oriented local regression algorithm that resulted in          to race and gender.\n",
      "considerable performance, but already highlighted concerns\n",
      "regarding demographic imbalances in the training data.                                         III. M ETHODOLOGY\n",
      "   Recent age estimation models are increasingly based on\n",
      "                                                                        We now present the methodology used in the evaluation.\n",
      "CNNs. Yan et al. [29], for example, reported impressive results\n",
      "                                                                     We discuss the age estimation models used, the experimental\n",
      "by building a multi-layer CNN model for feature extraction\n",
      "                                                                     datasets and setup and finally present the performance mea-\n",
      "and a Support Vector Machine (SVM) for classifying faces\n",
      "                                                                     sures used for our analysis.\n",
      "into different age groups. Levi et al. [19] presented a rela-\n",
      "                                                                        Age estimation models. We use the following (pre-trained)\n",
      "tively simple CNN architecture for age estimation that can\n",
      "                                                                     off-the-shelf age estimation models for the experiments:\n",
      "outperform previous methods if trained with sufficient training\n",
      "data. Niu et al. [22] presented a CNN model for joint feature          •   WideResNet: Our first model1 is based on the Wide\n",
      "learning and regression modeling, capable of making better use             Residual Network (WideResNet) architecture [24], [32],\n",
      "of large datasets. Overall, the use of CNNs greatly improved               but has two classification layers for age and gender.\n",
      "age estimation accuracy, however, problems with race and                   WideResNet are similar in spirit to residual networks, but\n",
      "gender disparities in results are still present with these models.         feature ResNet blocks with decreased depth and increased\n",
      "   Gender and ethnicity covariates. Covariates are variables               width. We use two variants of the WideResNet model\n",
      "that either increase intra-class variation or decrease inter-class         for our analysis: the first is trained on the UTKFace\n",
      "variation [1] and, hence, affect the performance of machine                dataset [33] and the second on the IMDB-WIKI dataset\n",
      "learning models. While pose, occlusion or illumination can                 [23]. We denote these two models as WideResNet-UTK\n",
      "often be controlled, other covariates like race and gender                 and WideResNet-IMDB, respectively. Both models are\n",
      "cannot. Early work on face-related tasks, such as the Local                trained from scratch using a multi-task learning objective\n",
      "Binary Pattern (LBP) model for demographics classification                 including both age estimation and gender recognition.\n",
      "proposed by Yang et al. [31], already had trouble dealing              •   FaceNet: Our second model2 is based on the FaceNet\n",
      "with ethnicity and gender. The face recognition meta-analysis              architecture [27], which is one of the first deep CNNs\n",
      "conducted by Lui et al. [20], examined 25 studies and showed               optimized for face recognition. The model is initialized\n",
      "that there is no general consensus on the influence of gender              with weights of a FaceNet model trained for face recog-\n",
      "and race. However, the study found that the uneven distribution            nition on the VGGFace2 dataset (featuring around 3.3\n",
      "of subjects across age groups in datasets is a big problem.                million faces and 9000 identities [5]). Similarly to the\n",
      "Datasets are dominated by younger subjects [15], since they                second WideResNet mode described above, the model\n",
      "rely on (typically student) volunteers [1]. Solutions were                 then trained (or better said fine-tuned) for the tasks of age\n",
      "later proposed to make the trained models more robust to                   estimation and gender recognition on the IMDB-WIKI\n",
      "problematic covariates, such as partitioning the datasets into             dataset.\n",
      "equal-sized sets with respect to gender and ethnicity [26] or        The models above were selected for our analysis because of\n",
      "using a framework for additional ethnicity and gender pre-           their state-of-the-art performance and the fact that two of the\n",
      "classification, as proposed by Guo et al. [13]. Abdurrahim           models have a different architecture, but were trained on the\n",
      "et al. [1] suggest that men and women have different local           same dataset (WideResNet-IMDB and FaceNet), while two\n",
      "features, however, girls and boys have similar craniofacial fea-     share the same architecture, but were trained on different\n",
      "tures. Most results confirm that women are harder to recognize       datasets (WideResNet-IMDB and WideResNet-UTK).\n",
      "than men, however, with age, the difference diminishes [21].           Experimental datasets. We select two popular datasets for\n",
      "The study in [20] was unable to determine which race or what         age estimation for the experiments:\n",
      "qualities prove to be a problem for current models, which is           •   The APPA-REAL dataset [2] contains 7, 591 images with\n",
      "a recurring issue among covariate analyses [12]. Drozdowski                associated real and apparent age labels. The age range\n",
      "et al. [9] present a comprehensive survey of the challenges                of subjects on the pictures is between 0 and 95 years.\n",
      "associated with algorithmic bias in biometric applications.                The dataset provides annotations with information about\n",
      "   Bias in age estimation models. A handful of existing                    various covariates of the pictures, including gender and\n",
      "studies investigated the issue of bias in age estimation models.           ethnicity [8]. The annotations partition the data into three\n",
      "Xing et al. [28], for example, analyzed the performance of                 race classes: Caucasian, Asian and Afro-American.\n",
      "their model across gender and ethnicity sub-groups. Alvi et            •   The UTKFace dataset [33] is a relatively large face image\n",
      "al. [4] suggested that training datasets that are not balanced             dataset with subjects aged from 0 to 116 years. The\n",
      "in terms of gender can lead to age estimation models that\n",
      "are gender-biased. Clapes et al. [8] showed that there is              1 Available   from https://github.com/yu4u/age-gender-estimation\n",
      "some consistent bias across various demographic groups when            2 Available   from https://github.com/BoyuanJiang/Age-Gender-Estimate-TF\n",
      "\n",
      "\n",
      "\n",
      "                                                                 831\n",
      "\f",
      "\n",
      "                                                                       Fig. 3: Schematic demonstration of the methodology used to\n",
      "                                                                       analyze gender and race bias of deep age estimation models.\n",
      "Fig. 2: Sample images from APPA-REAL (top) and UTKFace\n",
      "(bottom). Both datasets contain images of varying quality,\n",
      "different head poses, light settings, and facial expressions.          Error (MAE) scores, which serve as indicators of the average\n",
      "                                                                       performance of the age estimators:\n",
      "      dataset consists of 23, 708 face images captured “in-the-                                  1X\n",
      "                                                                                                         n\n",
      "      wild” that cover a large variety of poses, illumination,                            M AE =       |yj − ŷj |,                   (1)\n",
      "                                                                                                 n j=1\n",
      "      occlusions, resolution, and facial expressions. The images\n",
      "      are labelled by age, gender, and ethnicity and include           where n denotes the number of all test images, and ŷj and yj\n",
      "      five ethnicity categories: White, Black, Asian, Indian and       represent the predicted and the ground truth age, respectively.\n",
      "      Others. The ground truth of these labels was estimated             Additionally, we also report the Root Mean Squared Error\n",
      "      by the Deep Expectation (DEX) algorithm [25] and then            (RMSE), which emphasizes larger age estimation errors and\n",
      "      checked by human annotators.                                     penalizes them more:\n",
      "   Both datasets are widely used for age estimation and are                                       v\n",
      "                                                                                                  u1 n\n",
      "                                                                                                  u X\n",
      "free for non-commercial use. Since we use pre-trained off-the-\n",
      "shelf models for the analysis, no training data is required. We,                       RM SE = t           |yj − ŷj |2 ,          (2)\n",
      "                                                                                                     n j=1\n",
      "therefore, use all available image data from the two datasets\n",
      "as the test data for experimentation. A few example images             where n, ŷj and yj again represent the same variables as in\n",
      "from the two datasets are shown in Fig. 2.                             Eq. (1). The two metrics represent the literature standard when\n",
      "   Experimental setup. To evaluate age estimation perfor-              evaluating age estimation models [29], [11], [3], [22], [16].\n",
      "mance while also analysing race and gender bias, we partition\n",
      "the test data into different groups of interest, as also illustrated                   IV. E XPERIMENTAL RESULTS\n",
      "in Fig. 3. For analysing gender bias, we simply separate                  In this section we present our experimental procedure in\n",
      "images into male (M group) and female (F group) categories             analysing gender and race bias of the considered CNN models.\n",
      "based on the available image annotations. For analysing race           We start the section by describing our experimental setup and\n",
      "bias, we similarly generate race categories. The number of             the used performance metrics and then present our final results.\n",
      "race categories is defined by the available race labels in both           Baseline age estimation performance: In the first series of\n",
      "datasets. As already indicated above, the UTKFace dataset              experiments, we assess the performance of the three models\n",
      "provides 5 race labels, denoting White (W), Black (B), Asian           over all available test data of the UTKFace and APPA-REAL.\n",
      "(A), Indian (I), and Others (O) subjects. The APPA-REAL                We test all models on the APPA-REAL dataset, but exclude\n",
      "dataset only provides three separate race labels for Caucasian         the WideResNet-UTK models from the experiments on the\n",
      "(W), Afro-American (B) and Asian (A). Furthermore, we                  UTKFace dataset, because this datasets was used to train the\n",
      "generate combined gender-race sub-groups by additionally               model.\n",
      "separating each race group by gender. In doing so, we intend              The MAE and RMSE values reported in Tables I and II\n",
      "to produce specific results for these sub-groups and provide an        show estimation errors averaging between 6 and 10 years\n",
      "explicit comparison between them. This helps us determine if           in terms of MAE and between 9 and 14 years in terms\n",
      "any sub-group stands out and has a specifically large impact on        of RMSE. These results are a little above the current state-\n",
      "the performance of any particular group (e.g., whether white           of-the-art, which we ascribe to the preprocessing procedure,\n",
      "males affect the results for males the most). Since we can             where we do not explicitly align faces based on landmarks\n",
      "compare both genders within the same race group and vice               after the detection step. However, the absolute values of the\n",
      "versa, we can also investigate whether the models generate             performance metrics are not critical, as our focus in this study\n",
      "consistent deviations in performance throughout all groups,            is on the relative comparison of the performance scores across\n",
      "e.g., if one gender consistently over- or under-performs when          different demographic groups and sub-groups.\n",
      "compared to the opposite gender for a given race.                         Overall, we observe that the error scores for the APPA-\n",
      "   Performance metrics. In order to evaluate and compare               REAL dataset are lower than for the UTKFace dataset, which\n",
      "the performance of the selected age estimation models on               points to a difference in the difficulty of the two datasets and a\n",
      "various demographic sub-group, we report Mean Absolute                 better fit of the off-the-shelf models for the type of data present\n",
      "\n",
      "\n",
      "                                                                   832\n",
      "\f",
      "\n",
      "TABLE I: MAE and RMSE values (in years) for different race and gender groups. The groups are labelled with first letters\n",
      "for gender: Male (M) and Female (F), and race: White (W), Black (B), Asian (A), Indian (I) and Others (O). APPA-REAL\n",
      "does not have Indian (I) and Others (O) categories.\n",
      "                                                                     Subgroup Division & Performance Metrics\n",
      "                         Test                   Gender                                                Race\n",
      "      Model\n",
      "                        dataset     MAE    (yrs.)  RMSE    (yrs.)          MAE (yrs.)                        RMSE (yrs.)\n",
      "                                     M        F      M        F        W     B       A     I     O       W       B       A       I       O\n",
      "                     UTKFace          -       -       -       -         -     -      -     -      -       -       -       -      -        -\n",
      " WideResNet-UTK\n",
      "                     APPA-REAL      6.45    7.72    8.67    10.20     7.03  7.69 7.36      -      -     9.41    9.94    9.65     -        -\n",
      "                     UTKFace        8.83    8.91 12.10      11.90     9.79  7.71 9.56 8.02 6.99 13.60 11.10 13.40              11.00    9.39\n",
      " WideResNet-IMDB\n",
      "                     APPA-REAL      6.89    8.01    8.99    10.40     7.38  8.65 7.70      -      -     9.63   10.60 10.10       -        -\n",
      "                     UTKFace        8.22    8.20 11.20      11.50     8.22  7.25 10.4 7.66 7.68 11.20 10.10 14.30              10.20   10.70\n",
      " FaceNet\n",
      "                     APPA-REAL      7.69    7.95   10.70    11.10     7.79  8.23 7.85      -      -    10.90 10.50 10.60         -        -\n",
      "                     UTKFace        8.52    8.55    11.6     11.7     9.01  7.48 9.98 7.84 7.34 12.40 10.60             13.9   10.60   10.10\n",
      " Average\n",
      "                     APPA-REAL      7.01    7.89    9.45    10.57     7.40  8.19 7.64      -      -     9.98   10.30 10.10       -        -\n",
      "\n",
      "\n",
      "TABLE II: MAE scores (in years) for demographic sub-groups divided by both gender and race. The labels represent combined\n",
      "gender-race sub-groups with the first letter encoding the gender and the second letter the race - Male (M), Female (F) and\n",
      "White (W), Black (B), Asian (A) Indian (I) and Other (O), e.g., Male Asian - MA.\n",
      "                                                                    MAE of Race-Gender Divided Sub-Groups (yrs.)\n",
      "                        Model         Test dataset\n",
      "                                                      MW        FW    MB     FB     MA       FA    MI    FI      MO   FO\n",
      "                                      UTKFace           -         -     -     -       -       -    -      -       -     -\n",
      "                   WideResNet-UTK\n",
      "                                      APPA-REAL       6.33      7.75  7.47 7.89     7.36    7.37   -      -       -     -\n",
      "                                      UTKFace         9.08     10.70  7.72 7.69 11.30 8.10 8.26         7.71 6.99     6.98\n",
      "                   WideResNet-IMDB\n",
      "                                      APPA-REAL       6.80      7.98  8.13 9.12     7.47    7.89   -      -       -     -\n",
      "                                      UTKFace         7.89      8.61  7.13 7.36 11.90 9.06 7.84         7.42 7.42     7.90\n",
      "                   FaceNet\n",
      "                                      APPA-REAL       7.66      7.94  8.19 8.26     7.69    7.98   -      -       -     -\n",
      "                                      UTKFace         8.49      9.63  7.43 7.53 11.60 8.58 8.05         7.57 7.21     7.44\n",
      "                   Average\n",
      "                                      APPA-REAL       6.93      7.89  7.93 8.42     7.51    7.75   -      -       -     -\n",
      "\n",
      "\n",
      "\n",
      "in APPA-REAL. When comparing models, we notice that the                  (observe results for FaceNet) as well as the characteristics of\n",
      "two WideResNet models perform similarly regardless of the                the test images have a much larger impact on age estimation\n",
      "training data. The performance difference between the models             results in our experiments.\n",
      "is minimal with a slight edge for the WideResNet-UTK model.                 Race group comparison: When looking at the MAE and\n",
      "The FaceNet model, on the other hand, outperforms both                   RMSE scores for different race groups in Table I, we observe\n",
      "WideResNet models on the APPA-REAL data, but performs                    clear differences in performances of individual groups. The\n",
      "worse than WideResNet-IMDB on the UTKFace dataset.                       results are similar for all three considered models, but vary\n",
      "   Gender group comparison: Comparing the results reported               greatly among the two test datasets. The main reason for this is\n",
      "in Table I for each gender, we observe noticeable differences in         the inconsistent race partitioning between datasets, where the\n",
      "the calculated MAE and RMSE scores. Male subjects result in              race labels of the two datasets may not necessarily correspond\n",
      "more accurate age predictions with both WideResNet models                to subjects from the same races. For example, APPA-REAL\n",
      "when tested on the APPA-REAL dataset regardless of the data              does not have an Indian label, which suggest that Indians are\n",
      "used to train the models. Here, the MAE differences for the              likely part of the White (W) label. We therefore discuss results\n",
      "two genders are in the range of a 1.5 years. The results for the         separately for each of the two test datasets.\n",
      "FaceNet model show less divergence between genders, but still               On the APPA-REAL dataset the performance is consistent\n",
      "slightly favor male subjects over females. The performance               for all three models. The estimation errors are comparable for\n",
      "difference is significantly smaller on the UTKFace dataset. On           the White (W) and Asian (A) groups followed by the Black\n",
      "this dataset all models performs similarly for both genders with         (B) demographic group, where we observe between 0.4 and\n",
      "minimal differences in MAE and RMSE scores. Overall, we                  1.2 years larger MAE scores compared to the best performing\n",
      "observe that that age estimation is more accurate (or at least           race group. Interestingly, on the UTKFace dataset, we observe\n",
      "comparable) for male subjects than for female ones, which                a very different setting. Here, the Other (O) and Black (B) race\n",
      "may be related in part to the use of makeup, which affects               groups result in the lowest age estimation errors, followed by\n",
      "female facial appearance and consequently age estimation.                the Indian (I) race group. Here, the largest errors are observed\n",
      "Given the fact that UTKFace is approximately gender balanced             for the White (W) and Asian (A) groups. This observation is\n",
      "(with a ratio of 10 : 9 in favor of males), while IMDB-WIKI is           particularly interesting and points to the fact that other data\n",
      "not (a ratio of 14 : 10 in favor of males) the difference in the         characteristics have a much greater impact on age estimation\n",
      "performance cannot be ascribed to the training data. Instead,            performance than race. Our experiments did not identify\n",
      "it appears that the model architecture and training procedure            a consistent trend with respect to race-related performance\n",
      "\n",
      "\n",
      "                                                                     833\n",
      "\f",
      "\n",
      "variations, but point to the need for establishing consistent                  [2] E. Agustsson, R. Timofte, S. Escalera, X. Baro, I. Guyon, and R. Rothe.\n",
      "quality criteria (e.g., with respect to pose, illumination, image                  Apparent and real age estimation in still images with deep residual\n",
      "                                                                                   regressors on appa-real database. In FG, 2017.\n",
      "quality, etc.) across different demographic groups to be able                  [3] J. Alarifi, J. Fry, D. Dancey, and M. H. Yap. Understanding face age\n",
      "to compare age estimation performance across race groups                           estimation: humans and machine. In CITS, pages 1–5, 2019.\n",
      "                                                                               [4] M. Alvi, A. Zisserman, and C. Nellåker. Turning a blind eye: Explicit\n",
      "irrespective of other image-quality factors.                                       removal of biases and variation from deep neural network embeddings.\n",
      "   Gender-race sub-group comparison: In Table II we report                         In ECCV, 2018.\n",
      "MAE scores for sub-groups of subjects partitioned with respect                 [5] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vggface2:\n",
      "                                                                                   A dataset for recognising faces across pose and age. In FG, 2018.\n",
      "to race and gender. We do not report RMSE results for                          [6] K.-Y. Chang, C.-S. Chen, and Y.-P. Hung. Ordinal hyperplanes ranker\n",
      "this experiments to keep the table uncluttered. While most                         with cost sensitivities for age estimation. In CVPR.\n",
      "                                                                               [7] W.-L. Chao, J.-Z. Liu, and J.-J. Ding. Facial age estimation based on\n",
      "results are consistent with our previous findings, we notice                       label-sensitive learning and age-oriented regression. Pat. Rec., 46(3):628\n",
      "some exceptions. When comparing the gender-divided Asian                           – 641, 2013.\n",
      "(A) group results, we observe that male subjects performs                      [8] A. Clapés, O. Bilici, D. Temirova, E. Avots, G. Anbarjafari, and\n",
      "                                                                                   S. Escalera. From apparent to real age: gender, age, ethnic, makeup,\n",
      "a lot worse than female subjects on the UTKFace dataset                            and expression bias analysis in real age estimation. In CVPR-W, 2018.\n",
      "for both tested models, which affects the overall Asian (A)                    [9] P. Drozdowski, C. Rathgeb, A. Dantcheva, N. Damer, and C. Busch.\n",
      "                                                                                   Demographic bias in biometrics: A survey on an emerging challenge.\n",
      "group results discussed in the previous section. Other than                        IEEE Transactions on Technology and Society, 2020.\n",
      "this, male subjects produce better results than female subjects               [10] Y. Fu, G. Guo, and T. S. Huang. Age synthesis and estimation via faces:\n",
      "in all race categories (the Male Indian (MI) sub-group from                        A survey. IEEE TPAMI, 32(11):1955–1976, 2010.\n",
      "                                                                              [11] X. Geng, Z. Zhou, and K. Smith-Miles. Automatic age estimation based\n",
      "UTKFace dataset being the only additional exception with                           on facial aging patterns. IEEE TPAMI, 29(12):2234–2240, 2007.\n",
      "minor differences). Interestingly, the MA sub-groups also                     [12] K. Grm, W. Scheirer, and V. Štruc. Face hallucination using cascaded\n",
      "                                                                                   super-resolution and identity priors. IEEE TIP, 29(1):2150–2165, 2020.\n",
      "appears to have been the deciding group in the gender-oriented                [13] G. Guo and G. Mu. Human age estimation: What is the influence across\n",
      "experiments that balanced the performances of the FaceNet                          race and gender? In CVPR-W, pages 71–78, 2010.\n",
      "model on the UTKFace dataset, since we see that FaceNet                       [14] G. Guo, G. Mu, Y. Fu, and T. S. Huang. Human age estimation using\n",
      "                                                                                   bio-inspired features. In CVPR, pages 112–119, 2009.\n",
      "performs better for males than for females on all other sub-                  [15] W. H. Ho, P. Watters, and D. Verity. Are younger people more difficult\n",
      "groups. When examining results on the APPA-REAL dataset                            to identify or just a peer-to-peer effect. In CAIP, pages 351–359, 2007.\n",
      "                                                                              [16] J. C. J. Junior, C. Ozcinar, M. Marjanovic, X. Baró, G. Anbarjafari,\n",
      "we observe a comparable results between genders across all                         and S. Escalera. On the effect of age perception biases for real age\n",
      "races with the biggest gap between male and female subject                         regression. In FG, pages 1–8, 2019.\n",
      "being observed for the category of White (W) subjects.                        [17] Kwon, Y. H, and N. da Vitoria Lobo. Age classification from facial\n",
      "                                                                                   images. CVIU, 74(1):1–21, 1999.\n",
      "                                                                              [18] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different\n",
      "                          V. C ONCLUSION                                           classifiers for automatic age estimation. IEEE TSMC-B, 34(1):621–628,\n",
      "   In this study, we systematically analysed the performance                       2004.\n",
      "                                                                              [19] G. Levi and T. Hassner. Age and gender classification using convolu-\n",
      "of two off-the-shelf deep age estimation models based on face                      tional neural networks. In CVPR-W, pages 34–42, 2015.\n",
      "images from two publicly available datasets, UTKFace and                      [20] Y. M. Lui, D. Bolme, B. A. Draper, J. R. Beveridge, G. Givens, and\n",
      "                                                                                   P. J. Phillips. A meta-analysis of face recognition covariates. In BTAS,\n",
      "APPA-REAL. By performing age estimation on demographic                             pages 1–8, 2009.\n",
      "sub-categories of interest, we took a deeper look into race                   [21] M. Ngan, P. J. Grother, and M. Ngan. Face recognition vendor test\n",
      "and gender bias. Current datasets used when training and                           (FRVT) performance of automated gender classification algorithms.\n",
      "                                                                                   US Department of Commerce, National Institute of Standards and\n",
      "testing age estimation models do not represent all races and                       Technology, 2015.\n",
      "both genders equally. We observed a tendency in the tested                    [22] Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua. Ordinal regression\n",
      "models to perform better with male subjects than with female                       with multiple output cnn for age estimation. In CVPR, 2016.\n",
      "                                                                              [23] R. Rothe, R. Timofte, and L. V. Gool. Dex: Deep expectation of apparent\n",
      "ones, but did not identify a clear and consistent bias towards                     age from a single image. In ICCV-W, 2015.\n",
      "any particular race. Test dataset characteristics (especially                 [24] R. Rothe, R. Timofte, and L. V. Gool. Deep expectation of real and\n",
      "                                                                                   apparent age from a single image without facial landmarks. IJCV, 2016.\n",
      "for uncontrolled face images), such as image-quality, pose,                   [25] R. Rothe, R. Timofte, and L. Van Gool. Dex: Deep expectation of\n",
      "illumination, occlusion and the like appear to have a bigger                       apparent age from a single image. In ICCV-W, pages 10–15, 2015.\n",
      "                                                                              [26] M. B. F. Saavedra and R. S. Reı́llo. Evaluation methodologies for\n",
      "impact on age estimation performance than race. Nevertheless,                      security testing of biometric systems beyond technological evaluation.\n",
      "additional research is needed to better understand the factors                     Universidad Carlos III de Madrid, pages 43–57, 2013.\n",
      "affecting age estimation performance. A particular problem                    [27] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified\n",
      "                                                                                   embedding for face recognition and clustering. In CVPR, pages 815–\n",
      "here seems to be the lack of consistent quality boundaries                         823, 2015.\n",
      "across different demographic groups that would allow to                       [28] J. Xing, K. Li, W. Hu, C. Yuan, and H. Ling. Diagnosing deep learning\n",
      "evaluate the performance of current models on equal footing.                       models for high accuracy age estimation from a single image. Pat. Rec.,\n",
      "                                                                                   66:106–116, 2017.\n",
      "                                                                              [29] C. Yan, C. Lang, T. Wang, X. Du, and C. Zhang. Age estimation based\n",
      "                       ACKNOWLEDGMENTS                                             on convolutional neural network. In PR-CAMIP, pages 211–220, 2014.\n",
      "  This research was supported by the ARRS Research Pro-                       [30] S. Yan, H. Wang, X. Tang, and T. S. Huang. Learning auto-structured\n",
      "                                                                                   regressor from uncertain nonnegative labels. In ICCV, pages 1–8, 2007.\n",
      "grams P2-0250 (B) “Metrology and Biometric Systems”.                          [31] Z. Yang and H. Ai. Demographic classification with local binary\n",
      "                                                                                   patterns. In S.-W. Lee and S. Z. Li, editors, ICB, pages 464–473, 2007.\n",
      "                             R EFERENCES                                      [32] S. Zagoruyko and N. Komodakis. Wide residual networks. CoRR,\n",
      "                                                                                   abs/1605.07146, 2016.\n",
      " [1] S. H. Abdurrahim, S. A. Samad, and A. B. Huddin. Review on the effects   [33] S. Y. Zhang, Zhifei and H. Qi. Age progression/regression by conditional\n",
      "     of age, gender, and race demographics on automatic face recognition.          adversarial autoencoder. In CVPR, 2017.\n",
      "     The Visual Computer, 34(11):1617–1630, 2018.\n",
      "\n",
      "\n",
      "\n",
      "                                                                          834\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "for page in pdf:\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print specific page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Analysis of Race and Gender Bias in Deep Age\n",
      "                   Estimation Models\n",
      "                                          Andraž Puc, Vitomir Štruc, Klemen Grm\n",
      "                                  University of Ljubljana, Faculty of Electrical Engineering\n",
      "                                       Tržaška cesta 25, SI-1000 Ljubljana, Slovenia\n",
      "                                                    klemen.grm@fe.uni-lj.si\n",
      "\n",
      "\n",
      "   Abstract—Due to advances in deep learning and convolutional\n",
      "neural networks (CNNs) there has been significant progress in\n",
      "the field of visual age estimation from face images over recent\n",
      "years. While today’s models are able to achieve considerable age\n",
      "estimation accuracy, their behaviour, especially with respect to\n",
      "specific demographic groups is still not well understood. In this\n",
      "paper, we take a deeper look at CNN-based age estimation models\n",
      "and analyze their performance across different race and gender\n",
      "groups. We use two publicly available off-the-shelf age estimation\n",
      "models, i.e., FaceNet and WideResNet, for our study and analyze        Fig. 1: Age estimation from face images has progressed\n",
      "their performance on the UTKFace and APPA-REAL datasets.               considerably in recent years with state-of-the-art models pro-\n",
      "We partition face images into sub-groups based on race, gender         ducing highly accurate estimation results. In this paper we\n",
      "and combinations of race and gender. We then compare age\n",
      "estimation results and find that there are noticeable differences in\n",
      "                                                                       analyze and compare age estimation performance across dif-\n",
      "performance across demographics. Specifically, our results show        ferent demographic groups in terms of both gender and race.\n",
      "that age estimation accuracy is consistently higher for men than\n",
      "for women, while race does not appear to have consistent effects            for certain races, the age estimation models seem to favor\n",
      "on the tested models across different test datasets.\n",
      "                                                                            men over women in term of estimation errors in general.\n",
      "                      I. I NTRODUCTION                                    • We observe no consistent impact of race on age esti-\n",
      "\n",
      "   Age estimation from facial images (illustrated in Fig. 1)                mation accuracy. While different race groups produce\n",
      "has seen increased interest from the machine learning and                   consistent performance variations with all tested models,\n",
      "computer vision communities recently [3], [19], [22], [29]. The             these appear to be inconsistent between different test\n",
      "possibility of determining age from a face image automatically              datasets, suggesting that other nuisance factors affect\n",
      "and with high accuracy can facilitate applications with con-                results to a greater extent than race.\n",
      "siderable market potential, such as detecting minors for legal            The rest of the paper is structured as follows: In Section II\n",
      "purposes or adjusting application interfaces based on users’           we briefly review existing work related to our study. Next, we\n",
      "age. However, it is paramount to understand the behaviour              elaborate on the methodology used in the paper in Section III\n",
      "of existing age estimation models, especially with respect to          and discuss experimental findings in Section IV. We conclude\n",
      "their performance across different demographic groups, when            the paper with some final comments in Section V.\n",
      "deploying them in real-life applications .\n",
      "   While existing work has looked at the impact of demograph-                               II. R ELATED WORK\n",
      "ics when evaluating new models, e.g., [4], this has mostly been           In this section, we present prior work that relates to our\n",
      "a side result of the overall experimental evaluation. Studies          study. We first discuss existing techniques for age estimation,\n",
      "focusing specifically on demographic model bias, on the other          then review existing studies on the impact of gender and race\n",
      "hand, are still limited in the literature. In this paper, we try       in various face-related tasks and finally elaborate on existing\n",
      "to fill this gap and study the impact of race and gender on            work on bias in age estimation models.\n",
      "the accuracy of contemporary deep age estimation models.                  Age estimation. One of the early attempts at age estimation\n",
      "Specifically, we experiment with two pre-trained off-the-shelf         was presented by Kwon et al. in [17] and used an active\n",
      "age estimation models and evaluate their performance on                contour snakelet model that focused on wrinkles and simplified\n",
      "two publicly available datasets. The main contribution of our          the age estimation task into a binary classification problem.\n",
      "work are important findings that help to better understand             In [18], Lanitis et al. described an automatic age estimation\n",
      "age estimation models and their performance on different sub-          approach relying on Active Appearance Models (AAMs) to\n",
      "groups of subjects, such as:                                           jointly extract shape and texture information from an input\n",
      "   • We report results that suggest that age estimation with           face. Later Geng et al. [11] proposed a new approach by\n",
      "      the tested models is more accurate for male subjects than        modeling aging patterns with representative sub-spaces. Guo\n",
      "      for female subjects. While we observe opposite settings          et al. [14] used bio-inspired features (BIFs) and a multilayer\n",
      "\n",
      "\n",
      " 978-9-0827-9705-3                                                 830                                               EUSIPCO 2020\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pdf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subscript within page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     Analysis of Rac'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[0][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(''.join(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens can be indexed\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find word in token list using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'experiment'\n",
    "nltk.Text(tokens).count(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find five words in pdf, place word & count in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parsing issue\n",
    "# not all tokens are consistent in case\n",
    "tokens[23].lower(), tokens[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all tokens to lowercase\n",
    "low_tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no more conflict of case\n",
    "low_tokens[23].lower(), low_tokens[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists\n",
    "words = [\"accuracy\", \"findings\", \"experimental\", \"estimation\", \"doctor\"]\n",
    "word_summary = []\n",
    "low_word_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check every word in the 5 to search against the tokens\n",
    "# insert word/count in summary\n",
    "for word in words:\n",
    "    word_summary.append([word, nltk.Text(tokens).count(word)])\n",
    "word_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but utilizing lowercase tokens\n",
    "# results are different but important\n",
    "# ex. 'experimental' is 5 without lowercase and 7 with all lowercase\n",
    "for word in words:\n",
    "    low_word_summary.append([word, nltk.Text(low_tokens).count(word)])\n",
    "low_word_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create pandas dataframe for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(word_summary, columns = [\"word\", \"count\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low = pd.DataFrame(low_word_summary, columns = [\"word\", \"count\"])\n",
    "# demo start\n",
    "#df_low = df_low.set_index('word')\n",
    "# demo end\n",
    "df_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low.plot.bar(x='word', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Directory of PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = Path('test_pdf/').rglob('*.pdf')\n",
    "files = [file for file in pdf_folder]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# iterate every file in directory\n",
    "for file in files:\n",
    "    # open file\n",
    "    with open(file, 'rb') as f:\n",
    "        # conversion with pdftotext\n",
    "        multi_pdf = pdftotext.PDF(f)\n",
    "        # place current pdf text into list of tokens\n",
    "        tokens += nltk.word_tokenize(''.join(multi_pdf))\n",
    "\n",
    "# update tokens by setting all to lowercase,\n",
    "# removing stopwords,\n",
    "# removing non-alphanumeric\n",
    "tokens_removed = [word.lower() for word in tokens\n",
    "                  if word.lower() not in stopWords\n",
    "                  and word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize user summary list\n",
    "word_summary = []\n",
    "# create list with sublist [word, count]\n",
    "for word in range(len(words)):\n",
    "    word_summary.append([words[word], nltk.Text(tokens).count(words[word])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(tokens_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user specifies 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi = pd.DataFrame(word_summary, columns = [\"word\", \"count\"])\n",
    "df_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi.plot.bar(x='word', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK provides top 5\n",
    "* This allows the data to drive further work by looking solely at the top N words in the files processed.  By default the tokens are cleaned by removing entries in the NLTK library stopwords list as well as punctuation from the string library.\n",
    "* Utilizing this rather than user input allows the opportunity for supervised follow-up utilizing some terms from the tokens and additional to narrow in on \"hits\" in the dataset.\n",
    "* Since most_common() provides all entries it can be indexed as a traditional list to look anywhere in the list if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequencity distribution based off of the cleaned tokens\n",
    "fd = nltk.FreqDist(tokens_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK most_common(n) provides a list of n length with sublist [word, count]\n",
    "# create a dataframe utilizing the 5 most common words in the claned token list\n",
    "data = fd.most_common()\n",
    "df_fd = pd.DataFrame(data[:5], columns = [\"word\", \"count\"])\n",
    "df_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fd.plot.bar(x='word', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# TO-DO\n",
    "### immediate\n",
    "\n",
    "### long-term\n",
    "* Look into API for digital commons\n",
    "* Adjust visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
