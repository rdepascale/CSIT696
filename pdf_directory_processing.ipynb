{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Process a directory of PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('test_pdf/MedvedevaEtAl2019.pdf'),\n",
       " PosixPath('test_pdf/KDD97-003.pdf'),\n",
       " PosixPath('test_pdf/P99-1001.pdf'),\n",
       " PosixPath('test_pdf/10.1007978-3-319-67056-018.pdf'),\n",
       " PosixPath('test_pdf/dummy_test.pdf')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjust directory to point to location of files\n",
    "directory = 'test_pdf/'\n",
    "\n",
    "# create file list of pdf in directory\n",
    "pdf_folder = Path(directory).rglob('*.pdf')\n",
    "\n",
    "# create list of files and verify contents\n",
    "# should be 5 if using supplied 'test_pdf' directory\n",
    "files = [file for file in pdf_folder]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through each file and\n",
    "* Tokenize file text\n",
    "* Create consistent case `.lower()` for each token\n",
    "* Remove tokens from `nltk` library `english` stopwords\n",
    "* Remove non-`.isalpha()` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     Analysis of Race and Gender Bias in Deep Age\\n                   Estimation Models\\n                                          Andraž Puc, Vitomir Štruc, Klemen Grm\\n                                  University of Ljubljana, Faculty of Electrical Engineering\\n                                       Tržaška cesta 25, SI-1000 Ljubljana, Slovenia\\n                                                    klemen.grm@fe.uni-lj.si\\n\\n\\n   Abstract—Due to advances in deep learning and convolutional\\nneural networks (CNNs) there has been significant progress in\\nthe field of visual age estimation from face images over recent\\nyears. While today’s models are able to achieve considerable age\\nestimation accuracy, their behaviour, especially with respect to\\nspecific demographic groups is still not well understood. In this\\npaper, we take a deeper look at CNN-based age estimation models\\nand analyze their performance across different race and gender\\ngroups. We use two publicly available off-the-shelf age estimation\\nmodels, i.e., FaceNet and WideResNet, for our study and analyze        Fig. 1: Age estimation from face images has progressed\\ntheir performance on the UTKFace and APPA-REAL datasets.               considerably in recent years with state-of-the-art models pro-\\nWe partition face images into sub-groups based on race, gender         ducing highly accurate estimation results. In this paper we\\nand combinations of race and gender. We then compare age\\nestimation results and find that there are noticeable differences in\\n                                                                       analyze and compare age estimation performance across dif-\\nperformance across demographics. Specifically, our results show        ferent demographic groups in terms of both gender and race.\\nthat age estimation accuracy is consistently higher for men than\\nfor women, while race does not appear to have consistent effects            for certain races, the age estimation models seem to favor\\non the tested models across different test datasets.\\n                                                                            men over women in term of estimation errors in general.\\n                      I. I NTRODUCTION                                    • We observe no consistent impact of race on age esti-\\n\\n   Age estimation from facial images (illustrated in Fig. 1)                mation accuracy. While different race groups produce\\nhas seen increased interest from the machine learning and                   consistent performance variations with all tested models,\\ncomputer vision communities recently [3], [19], [22], [29]. The             these appear to be inconsistent between different test\\npossibility of determining age from a face image automatically              datasets, suggesting that other nuisance factors affect\\nand with high accuracy can facilitate applications with con-                results to a greater extent than race.\\nsiderable market potential, such as detecting minors for legal            The rest of the paper is structured as follows: In Section II\\npurposes or adjusting application interfaces based on users’           we briefly review existing work related to our study. Next, we\\nage. However, it is paramount to understand the behaviour              elaborate on the methodology used in the paper in Section III\\nof existing age estimation models, especially with respect to          and discuss experimental findings in Section IV. We conclude\\ntheir performance across different demographic groups, when            the paper with some final comments in Section V.\\ndeploying them in real-life applications .\\n   While existing work has looked at the impact of demograph-                               II. R ELATED WORK\\nics when evaluating new models, e.g., [4], this has mostly been           In this section, we present prior work that relates to our\\na side result of the overall experimental evaluation. Studies          study. We first discuss existing techniques for age estimation,\\nfocusing specifically on demographic model bias, on the other          then review existing studies on the impact of gender and race\\nhand, are still limited in the literature. In this paper, we try       in various face-related tasks and finally elaborate on existing\\nto fill this gap and study the impact of race and gender on            work on bias in age estimation models.\\nthe accuracy of contemporary deep age estimation models.                  Age estimation. One of the early attempts at age estimation\\nSpecifically, we experiment with two pre-trained off-the-shelf         was presented by Kwon et al. in [17] and used an active\\nage estimation models and evaluate their performance on                contour snakelet model that focused on wrinkles and simplified\\ntwo publicly available datasets. The main contribution of our          the age estimation task into a binary classification problem.\\nwork are important findings that help to better understand             In [18], Lanitis et al. described an automatic age estimation\\nage estimation models and their performance on different sub-          approach relying on Active Appearance Models (AAMs) to\\ngroups of subjects, such as:                                           jointly extract shape and texture information from an input\\n   • We report results that suggest that age estimation with           face. Later Geng et al. [11] proposed a new approach by\\n      the tested models is more accurate for male subjects than        modeling aging patterns with representative sub-spaces. Guo\\n      for female subjects. While we observe opposite settings          et al. [14] used bio-inspired features (BIFs) and a multilayer\\n\\n\\n 978-9-0827-9705-3                                                 830                                               EUSIPCO 2020\\n\\x0cHMAX model for age estimation. Chang et al. [6] proposed             relating the performance of apparent and real-age estimation\\nreplacing traditional multi-class labels with new ordinally ar-      tasks. In these paper we contribute to a better understanding\\nranged labels [10], [30] and developed a cost-sensitive ordinal      of the bias of age estimation models with an analysis of the\\nranking framework for age estimation. Chao et al. [7] proposed       performance of two recent deep learning models with respect\\nan age-oriented local regression algorithm that resulted in          to race and gender.\\nconsiderable performance, but already highlighted concerns\\nregarding demographic imbalances in the training data.                                         III. M ETHODOLOGY\\n   Recent age estimation models are increasingly based on\\n                                                                        We now present the methodology used in the evaluation.\\nCNNs. Yan et al. [29], for example, reported impressive results\\n                                                                     We discuss the age estimation models used, the experimental\\nby building a multi-layer CNN model for feature extraction\\n                                                                     datasets and setup and finally present the performance mea-\\nand a Support Vector Machine (SVM) for classifying faces\\n                                                                     sures used for our analysis.\\ninto different age groups. Levi et al. [19] presented a rela-\\n                                                                        Age estimation models. We use the following (pre-trained)\\ntively simple CNN architecture for age estimation that can\\n                                                                     off-the-shelf age estimation models for the experiments:\\noutperform previous methods if trained with sufficient training\\ndata. Niu et al. [22] presented a CNN model for joint feature          •   WideResNet: Our first model1 is based on the Wide\\nlearning and regression modeling, capable of making better use             Residual Network (WideResNet) architecture [24], [32],\\nof large datasets. Overall, the use of CNNs greatly improved               but has two classification layers for age and gender.\\nage estimation accuracy, however, problems with race and                   WideResNet are similar in spirit to residual networks, but\\ngender disparities in results are still present with these models.         feature ResNet blocks with decreased depth and increased\\n   Gender and ethnicity covariates. Covariates are variables               width. We use two variants of the WideResNet model\\nthat either increase intra-class variation or decrease inter-class         for our analysis: the first is trained on the UTKFace\\nvariation [1] and, hence, affect the performance of machine                dataset [33] and the second on the IMDB-WIKI dataset\\nlearning models. While pose, occlusion or illumination can                 [23]. We denote these two models as WideResNet-UTK\\noften be controlled, other covariates like race and gender                 and WideResNet-IMDB, respectively. Both models are\\ncannot. Early work on face-related tasks, such as the Local                trained from scratch using a multi-task learning objective\\nBinary Pattern (LBP) model for demographics classification                 including both age estimation and gender recognition.\\nproposed by Yang et al. [31], already had trouble dealing              •   FaceNet: Our second model2 is based on the FaceNet\\nwith ethnicity and gender. The face recognition meta-analysis              architecture [27], which is one of the first deep CNNs\\nconducted by Lui et al. [20], examined 25 studies and showed               optimized for face recognition. The model is initialized\\nthat there is no general consensus on the influence of gender              with weights of a FaceNet model trained for face recog-\\nand race. However, the study found that the uneven distribution            nition on the VGGFace2 dataset (featuring around 3.3\\nof subjects across age groups in datasets is a big problem.                million faces and 9000 identities [5]). Similarly to the\\nDatasets are dominated by younger subjects [15], since they                second WideResNet mode described above, the model\\nrely on (typically student) volunteers [1]. Solutions were                 then trained (or better said fine-tuned) for the tasks of age\\nlater proposed to make the trained models more robust to                   estimation and gender recognition on the IMDB-WIKI\\nproblematic covariates, such as partitioning the datasets into             dataset.\\nequal-sized sets with respect to gender and ethnicity [26] or        The models above were selected for our analysis because of\\nusing a framework for additional ethnicity and gender pre-           their state-of-the-art performance and the fact that two of the\\nclassification, as proposed by Guo et al. [13]. Abdurrahim           models have a different architecture, but were trained on the\\net al. [1] suggest that men and women have different local           same dataset (WideResNet-IMDB and FaceNet), while two\\nfeatures, however, girls and boys have similar craniofacial fea-     share the same architecture, but were trained on different\\ntures. Most results confirm that women are harder to recognize       datasets (WideResNet-IMDB and WideResNet-UTK).\\nthan men, however, with age, the difference diminishes [21].           Experimental datasets. We select two popular datasets for\\nThe study in [20] was unable to determine which race or what         age estimation for the experiments:\\nqualities prove to be a problem for current models, which is           •   The APPA-REAL dataset [2] contains 7, 591 images with\\na recurring issue among covariate analyses [12]. Drozdowski                associated real and apparent age labels. The age range\\net al. [9] present a comprehensive survey of the challenges                of subjects on the pictures is between 0 and 95 years.\\nassociated with algorithmic bias in biometric applications.                The dataset provides annotations with information about\\n   Bias in age estimation models. A handful of existing                    various covariates of the pictures, including gender and\\nstudies investigated the issue of bias in age estimation models.           ethnicity [8]. The annotations partition the data into three\\nXing et al. [28], for example, analyzed the performance of                 race classes: Caucasian, Asian and Afro-American.\\ntheir model across gender and ethnicity sub-groups. Alvi et            •   The UTKFace dataset [33] is a relatively large face image\\nal. [4] suggested that training datasets that are not balanced             dataset with subjects aged from 0 to 116 years. The\\nin terms of gender can lead to age estimation models that\\nare gender-biased. Clapes et al. [8] showed that there is              1 Available   from https://github.com/yu4u/age-gender-estimation\\nsome consistent bias across various demographic groups when            2 Available   from https://github.com/BoyuanJiang/Age-Gender-Estimate-TF\\n\\n\\n\\n                                                                 831\\n\\x0c                                                                       Fig. 3: Schematic demonstration of the methodology used to\\n                                                                       analyze gender and race bias of deep age estimation models.\\nFig. 2: Sample images from APPA-REAL (top) and UTKFace\\n(bottom). Both datasets contain images of varying quality,\\ndifferent head poses, light settings, and facial expressions.          Error (MAE) scores, which serve as indicators of the average\\n                                                                       performance of the age estimators:\\n      dataset consists of 23, 708 face images captured “in-the-                                  1X\\n                                                                                                         n\\n      wild” that cover a large variety of poses, illumination,                            M AE =       |yj − ŷj |,                   (1)\\n                                                                                                 n j=1\\n      occlusions, resolution, and facial expressions. The images\\n      are labelled by age, gender, and ethnicity and include           where n denotes the number of all test images, and ŷj and yj\\n      five ethnicity categories: White, Black, Asian, Indian and       represent the predicted and the ground truth age, respectively.\\n      Others. The ground truth of these labels was estimated             Additionally, we also report the Root Mean Squared Error\\n      by the Deep Expectation (DEX) algorithm [25] and then            (RMSE), which emphasizes larger age estimation errors and\\n      checked by human annotators.                                     penalizes them more:\\n   Both datasets are widely used for age estimation and are                                       v\\n                                                                                                  u1 n\\n                                                                                                  u X\\nfree for non-commercial use. Since we use pre-trained off-the-\\nshelf models for the analysis, no training data is required. We,                       RM SE = t           |yj − ŷj |2 ,          (2)\\n                                                                                                     n j=1\\ntherefore, use all available image data from the two datasets\\nas the test data for experimentation. A few example images             where n, ŷj and yj again represent the same variables as in\\nfrom the two datasets are shown in Fig. 2.                             Eq. (1). The two metrics represent the literature standard when\\n   Experimental setup. To evaluate age estimation perfor-              evaluating age estimation models [29], [11], [3], [22], [16].\\nmance while also analysing race and gender bias, we partition\\nthe test data into different groups of interest, as also illustrated                   IV. E XPERIMENTAL RESULTS\\nin Fig. 3. For analysing gender bias, we simply separate                  In this section we present our experimental procedure in\\nimages into male (M group) and female (F group) categories             analysing gender and race bias of the considered CNN models.\\nbased on the available image annotations. For analysing race           We start the section by describing our experimental setup and\\nbias, we similarly generate race categories. The number of             the used performance metrics and then present our final results.\\nrace categories is defined by the available race labels in both           Baseline age estimation performance: In the first series of\\ndatasets. As already indicated above, the UTKFace dataset              experiments, we assess the performance of the three models\\nprovides 5 race labels, denoting White (W), Black (B), Asian           over all available test data of the UTKFace and APPA-REAL.\\n(A), Indian (I), and Others (O) subjects. The APPA-REAL                We test all models on the APPA-REAL dataset, but exclude\\ndataset only provides three separate race labels for Caucasian         the WideResNet-UTK models from the experiments on the\\n(W), Afro-American (B) and Asian (A). Furthermore, we                  UTKFace dataset, because this datasets was used to train the\\ngenerate combined gender-race sub-groups by additionally               model.\\nseparating each race group by gender. In doing so, we intend              The MAE and RMSE values reported in Tables I and II\\nto produce specific results for these sub-groups and provide an        show estimation errors averaging between 6 and 10 years\\nexplicit comparison between them. This helps us determine if           in terms of MAE and between 9 and 14 years in terms\\nany sub-group stands out and has a specifically large impact on        of RMSE. These results are a little above the current state-\\nthe performance of any particular group (e.g., whether white           of-the-art, which we ascribe to the preprocessing procedure,\\nmales affect the results for males the most). Since we can             where we do not explicitly align faces based on landmarks\\ncompare both genders within the same race group and vice               after the detection step. However, the absolute values of the\\nversa, we can also investigate whether the models generate             performance metrics are not critical, as our focus in this study\\nconsistent deviations in performance throughout all groups,            is on the relative comparison of the performance scores across\\ne.g., if one gender consistently over- or under-performs when          different demographic groups and sub-groups.\\ncompared to the opposite gender for a given race.                         Overall, we observe that the error scores for the APPA-\\n   Performance metrics. In order to evaluate and compare               REAL dataset are lower than for the UTKFace dataset, which\\nthe performance of the selected age estimation models on               points to a difference in the difficulty of the two datasets and a\\nvarious demographic sub-group, we report Mean Absolute                 better fit of the off-the-shelf models for the type of data present\\n\\n\\n                                                                   832\\n\\x0cTABLE I: MAE and RMSE values (in years) for different race and gender groups. The groups are labelled with first letters\\nfor gender: Male (M) and Female (F), and race: White (W), Black (B), Asian (A), Indian (I) and Others (O). APPA-REAL\\ndoes not have Indian (I) and Others (O) categories.\\n                                                                     Subgroup Division & Performance Metrics\\n                         Test                   Gender                                                Race\\n      Model\\n                        dataset     MAE    (yrs.)  RMSE    (yrs.)          MAE (yrs.)                        RMSE (yrs.)\\n                                     M        F      M        F        W     B       A     I     O       W       B       A       I       O\\n                     UTKFace          -       -       -       -         -     -      -     -      -       -       -       -      -        -\\n WideResNet-UTK\\n                     APPA-REAL      6.45    7.72    8.67    10.20     7.03  7.69 7.36      -      -     9.41    9.94    9.65     -        -\\n                     UTKFace        8.83    8.91 12.10      11.90     9.79  7.71 9.56 8.02 6.99 13.60 11.10 13.40              11.00    9.39\\n WideResNet-IMDB\\n                     APPA-REAL      6.89    8.01    8.99    10.40     7.38  8.65 7.70      -      -     9.63   10.60 10.10       -        -\\n                     UTKFace        8.22    8.20 11.20      11.50     8.22  7.25 10.4 7.66 7.68 11.20 10.10 14.30              10.20   10.70\\n FaceNet\\n                     APPA-REAL      7.69    7.95   10.70    11.10     7.79  8.23 7.85      -      -    10.90 10.50 10.60         -        -\\n                     UTKFace        8.52    8.55    11.6     11.7     9.01  7.48 9.98 7.84 7.34 12.40 10.60             13.9   10.60   10.10\\n Average\\n                     APPA-REAL      7.01    7.89    9.45    10.57     7.40  8.19 7.64      -      -     9.98   10.30 10.10       -        -\\n\\n\\nTABLE II: MAE scores (in years) for demographic sub-groups divided by both gender and race. The labels represent combined\\ngender-race sub-groups with the first letter encoding the gender and the second letter the race - Male (M), Female (F) and\\nWhite (W), Black (B), Asian (A) Indian (I) and Other (O), e.g., Male Asian - MA.\\n                                                                    MAE of Race-Gender Divided Sub-Groups (yrs.)\\n                        Model         Test dataset\\n                                                      MW        FW    MB     FB     MA       FA    MI    FI      MO   FO\\n                                      UTKFace           -         -     -     -       -       -    -      -       -     -\\n                   WideResNet-UTK\\n                                      APPA-REAL       6.33      7.75  7.47 7.89     7.36    7.37   -      -       -     -\\n                                      UTKFace         9.08     10.70  7.72 7.69 11.30 8.10 8.26         7.71 6.99     6.98\\n                   WideResNet-IMDB\\n                                      APPA-REAL       6.80      7.98  8.13 9.12     7.47    7.89   -      -       -     -\\n                                      UTKFace         7.89      8.61  7.13 7.36 11.90 9.06 7.84         7.42 7.42     7.90\\n                   FaceNet\\n                                      APPA-REAL       7.66      7.94  8.19 8.26     7.69    7.98   -      -       -     -\\n                                      UTKFace         8.49      9.63  7.43 7.53 11.60 8.58 8.05         7.57 7.21     7.44\\n                   Average\\n                                      APPA-REAL       6.93      7.89  7.93 8.42     7.51    7.75   -      -       -     -\\n\\n\\n\\nin APPA-REAL. When comparing models, we notice that the                  (observe results for FaceNet) as well as the characteristics of\\ntwo WideResNet models perform similarly regardless of the                the test images have a much larger impact on age estimation\\ntraining data. The performance difference between the models             results in our experiments.\\nis minimal with a slight edge for the WideResNet-UTK model.                 Race group comparison: When looking at the MAE and\\nThe FaceNet model, on the other hand, outperforms both                   RMSE scores for different race groups in Table I, we observe\\nWideResNet models on the APPA-REAL data, but performs                    clear differences in performances of individual groups. The\\nworse than WideResNet-IMDB on the UTKFace dataset.                       results are similar for all three considered models, but vary\\n   Gender group comparison: Comparing the results reported               greatly among the two test datasets. The main reason for this is\\nin Table I for each gender, we observe noticeable differences in         the inconsistent race partitioning between datasets, where the\\nthe calculated MAE and RMSE scores. Male subjects result in              race labels of the two datasets may not necessarily correspond\\nmore accurate age predictions with both WideResNet models                to subjects from the same races. For example, APPA-REAL\\nwhen tested on the APPA-REAL dataset regardless of the data              does not have an Indian label, which suggest that Indians are\\nused to train the models. Here, the MAE differences for the              likely part of the White (W) label. We therefore discuss results\\ntwo genders are in the range of a 1.5 years. The results for the         separately for each of the two test datasets.\\nFaceNet model show less divergence between genders, but still               On the APPA-REAL dataset the performance is consistent\\nslightly favor male subjects over females. The performance               for all three models. The estimation errors are comparable for\\ndifference is significantly smaller on the UTKFace dataset. On           the White (W) and Asian (A) groups followed by the Black\\nthis dataset all models performs similarly for both genders with         (B) demographic group, where we observe between 0.4 and\\nminimal differences in MAE and RMSE scores. Overall, we                  1.2 years larger MAE scores compared to the best performing\\nobserve that that age estimation is more accurate (or at least           race group. Interestingly, on the UTKFace dataset, we observe\\ncomparable) for male subjects than for female ones, which                a very different setting. Here, the Other (O) and Black (B) race\\nmay be related in part to the use of makeup, which affects               groups result in the lowest age estimation errors, followed by\\nfemale facial appearance and consequently age estimation.                the Indian (I) race group. Here, the largest errors are observed\\nGiven the fact that UTKFace is approximately gender balanced             for the White (W) and Asian (A) groups. This observation is\\n(with a ratio of 10 : 9 in favor of males), while IMDB-WIKI is           particularly interesting and points to the fact that other data\\nnot (a ratio of 14 : 10 in favor of males) the difference in the         characteristics have a much greater impact on age estimation\\nperformance cannot be ascribed to the training data. Instead,            performance than race. Our experiments did not identify\\nit appears that the model architecture and training procedure            a consistent trend with respect to race-related performance\\n\\n\\n                                                                     833\\n\\x0cvariations, but point to the need for establishing consistent                  [2] E. Agustsson, R. Timofte, S. Escalera, X. Baro, I. Guyon, and R. Rothe.\\nquality criteria (e.g., with respect to pose, illumination, image                  Apparent and real age estimation in still images with deep residual\\n                                                                                   regressors on appa-real database. In FG, 2017.\\nquality, etc.) across different demographic groups to be able                  [3] J. Alarifi, J. Fry, D. Dancey, and M. H. Yap. Understanding face age\\nto compare age estimation performance across race groups                           estimation: humans and machine. In CITS, pages 1–5, 2019.\\n                                                                               [4] M. Alvi, A. Zisserman, and C. Nellåker. Turning a blind eye: Explicit\\nirrespective of other image-quality factors.                                       removal of biases and variation from deep neural network embeddings.\\n   Gender-race sub-group comparison: In Table II we report                         In ECCV, 2018.\\nMAE scores for sub-groups of subjects partitioned with respect                 [5] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vggface2:\\n                                                                                   A dataset for recognising faces across pose and age. In FG, 2018.\\nto race and gender. We do not report RMSE results for                          [6] K.-Y. Chang, C.-S. Chen, and Y.-P. Hung. Ordinal hyperplanes ranker\\nthis experiments to keep the table uncluttered. While most                         with cost sensitivities for age estimation. In CVPR.\\n                                                                               [7] W.-L. Chao, J.-Z. Liu, and J.-J. Ding. Facial age estimation based on\\nresults are consistent with our previous findings, we notice                       label-sensitive learning and age-oriented regression. Pat. Rec., 46(3):628\\nsome exceptions. When comparing the gender-divided Asian                           – 641, 2013.\\n(A) group results, we observe that male subjects performs                      [8] A. Clapés, O. Bilici, D. Temirova, E. Avots, G. Anbarjafari, and\\n                                                                                   S. Escalera. From apparent to real age: gender, age, ethnic, makeup,\\na lot worse than female subjects on the UTKFace dataset                            and expression bias analysis in real age estimation. In CVPR-W, 2018.\\nfor both tested models, which affects the overall Asian (A)                    [9] P. Drozdowski, C. Rathgeb, A. Dantcheva, N. Damer, and C. Busch.\\n                                                                                   Demographic bias in biometrics: A survey on an emerging challenge.\\ngroup results discussed in the previous section. Other than                        IEEE Transactions on Technology and Society, 2020.\\nthis, male subjects produce better results than female subjects               [10] Y. Fu, G. Guo, and T. S. Huang. Age synthesis and estimation via faces:\\nin all race categories (the Male Indian (MI) sub-group from                        A survey. IEEE TPAMI, 32(11):1955–1976, 2010.\\n                                                                              [11] X. Geng, Z. Zhou, and K. Smith-Miles. Automatic age estimation based\\nUTKFace dataset being the only additional exception with                           on facial aging patterns. IEEE TPAMI, 29(12):2234–2240, 2007.\\nminor differences). Interestingly, the MA sub-groups also                     [12] K. Grm, W. Scheirer, and V. Štruc. Face hallucination using cascaded\\n                                                                                   super-resolution and identity priors. IEEE TIP, 29(1):2150–2165, 2020.\\nappears to have been the deciding group in the gender-oriented                [13] G. Guo and G. Mu. Human age estimation: What is the influence across\\nexperiments that balanced the performances of the FaceNet                          race and gender? In CVPR-W, pages 71–78, 2010.\\nmodel on the UTKFace dataset, since we see that FaceNet                       [14] G. Guo, G. Mu, Y. Fu, and T. S. Huang. Human age estimation using\\n                                                                                   bio-inspired features. In CVPR, pages 112–119, 2009.\\nperforms better for males than for females on all other sub-                  [15] W. H. Ho, P. Watters, and D. Verity. Are younger people more difficult\\ngroups. When examining results on the APPA-REAL dataset                            to identify or just a peer-to-peer effect. In CAIP, pages 351–359, 2007.\\n                                                                              [16] J. C. J. Junior, C. Ozcinar, M. Marjanovic, X. Baró, G. Anbarjafari,\\nwe observe a comparable results between genders across all                         and S. Escalera. On the effect of age perception biases for real age\\nraces with the biggest gap between male and female subject                         regression. In FG, pages 1–8, 2019.\\nbeing observed for the category of White (W) subjects.                        [17] Kwon, Y. H, and N. da Vitoria Lobo. Age classification from facial\\n                                                                                   images. CVIU, 74(1):1–21, 1999.\\n                                                                              [18] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different\\n                          V. C ONCLUSION                                           classifiers for automatic age estimation. IEEE TSMC-B, 34(1):621–628,\\n   In this study, we systematically analysed the performance                       2004.\\n                                                                              [19] G. Levi and T. Hassner. Age and gender classification using convolu-\\nof two off-the-shelf deep age estimation models based on face                      tional neural networks. In CVPR-W, pages 34–42, 2015.\\nimages from two publicly available datasets, UTKFace and                      [20] Y. M. Lui, D. Bolme, B. A. Draper, J. R. Beveridge, G. Givens, and\\n                                                                                   P. J. Phillips. A meta-analysis of face recognition covariates. In BTAS,\\nAPPA-REAL. By performing age estimation on demographic                             pages 1–8, 2009.\\nsub-categories of interest, we took a deeper look into race                   [21] M. Ngan, P. J. Grother, and M. Ngan. Face recognition vendor test\\nand gender bias. Current datasets used when training and                           (FRVT) performance of automated gender classification algorithms.\\n                                                                                   US Department of Commerce, National Institute of Standards and\\ntesting age estimation models do not represent all races and                       Technology, 2015.\\nboth genders equally. We observed a tendency in the tested                    [22] Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua. Ordinal regression\\nmodels to perform better with male subjects than with female                       with multiple output cnn for age estimation. In CVPR, 2016.\\n                                                                              [23] R. Rothe, R. Timofte, and L. V. Gool. Dex: Deep expectation of apparent\\nones, but did not identify a clear and consistent bias towards                     age from a single image. In ICCV-W, 2015.\\nany particular race. Test dataset characteristics (especially                 [24] R. Rothe, R. Timofte, and L. V. Gool. Deep expectation of real and\\n                                                                                   apparent age from a single image without facial landmarks. IJCV, 2016.\\nfor uncontrolled face images), such as image-quality, pose,                   [25] R. Rothe, R. Timofte, and L. Van Gool. Dex: Deep expectation of\\nillumination, occlusion and the like appear to have a bigger                       apparent age from a single image. In ICCV-W, pages 10–15, 2015.\\n                                                                              [26] M. B. F. Saavedra and R. S. Reı́llo. Evaluation methodologies for\\nimpact on age estimation performance than race. Nevertheless,                      security testing of biometric systems beyond technological evaluation.\\nadditional research is needed to better understand the factors                     Universidad Carlos III de Madrid, pages 43–57, 2013.\\naffecting age estimation performance. A particular problem                    [27] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified\\n                                                                                   embedding for face recognition and clustering. In CVPR, pages 815–\\nhere seems to be the lack of consistent quality boundaries                         823, 2015.\\nacross different demographic groups that would allow to                       [28] J. Xing, K. Li, W. Hu, C. Yuan, and H. Ling. Diagnosing deep learning\\nevaluate the performance of current models on equal footing.                       models for high accuracy age estimation from a single image. Pat. Rec.,\\n                                                                                   66:106–116, 2017.\\n                                                                              [29] C. Yan, C. Lang, T. Wang, X. Du, and C. Zhang. Age estimation based\\n                       ACKNOWLEDGMENTS                                             on convolutional neural network. In PR-CAMIP, pages 211–220, 2014.\\n  This research was supported by the ARRS Research Pro-                       [30] S. Yan, H. Wang, X. Tang, and T. S. Huang. Learning auto-structured\\n                                                                                   regressor from uncertain nonnegative labels. In ICCV, pages 1–8, 2007.\\ngrams P2-0250 (B) “Metrology and Biometric Systems”.                          [31] Z. Yang and H. Ai. Demographic classification with local binary\\n                                                                                   patterns. In S.-W. Lee and S. Z. Li, editors, ICB, pages 464–473, 2007.\\n                             R EFERENCES                                      [32] S. Zagoruyko and N. Komodakis. Wide residual networks. CoRR,\\n                                                                                   abs/1605.07146, 2016.\\n [1] S. H. Abdurrahim, S. A. Samad, and A. B. Huddin. Review on the effects   [33] S. Y. Zhang, Zhifei and H. Qi. Age progression/regression by conditional\\n     of age, gender, and race demographics on automatic face recognition.          adversarial autoencoder. In CVPR, 2017.\\n     The Visual Computer, 34(11):1617–1630, 2018.\\n\\n\\n\\n                                                                          834\\n\\x0c'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(multi_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "multi_corpus = []\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# iterate every file in directory\n",
    "for file in files:\n",
    "    # open file\n",
    "    with open(file, 'rb') as f:\n",
    "        # conversion with pdftotext\n",
    "        multi_pdf = pdftotext.PDF(f)\n",
    "        multi_corpus.append(''.join(multi_pdf))\n",
    "        # place current pdf text into list of tokens\n",
    "        tokens += nltk.word_tokenize(''.join(multi_pdf))\n",
    "        corpus.append(tokens)\n",
    "\n",
    "# update tokens by setting all to lowercase,\n",
    "# removing stopwords,\n",
    "# removing non-alphanumeric\n",
    "tokens_removed = [word.lower() for word in tokens\n",
    "                  if word.lower() not in stopWords\n",
    "                  and word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus), corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify removal of stopwords by displaying initial `token` lenght, `tokens_removed` length and difference of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(tokens_removed), len(tokens)-len(tokens_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## user directed word search\n",
    "* alter `words` list to include desired search terms as list of type string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize user summary list\n",
    "words = [\"\"]\n",
    "word_summary = []\n",
    "# create list with sublist [word, count]\n",
    "for word in range(len(words)):\n",
    "    word_summary.append([words[word], nltk.Text(tokens).count(words[word])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user specifies 5 words\n",
    "* utilizes `pandas` library to create dataframe of `word` and `count` as well as simple bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi = pd.DataFrame(word_summary, columns = [\"word\", \"count\"])\n",
    "df_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi.plot.bar(x='word', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK provides top 5\n",
    "* This allows the data to drive further work by looking solely at the top N words in the files processed.\n",
    "* By default the tokens are cleaned by removing `tokens` in the `nltk` library `stopwords` English list, as well as punctuation and non-alphanumeric `tokens` from the `string` methods.\n",
    "* `nltk` directed search allows the opportunity for supervised follow-up utilizing inference from `tokens` provided and additional user input to narrow in on \"hits\" in the dataset.\n",
    "* Since `most_common()` method provides all entries it can be indexed as a traditional list to look anywhere in the list if desired.\n",
    " * Alternately `most_common(n)` can be used where `n` is the number of words to search for if concerned about slowdown via processing too many tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a frequencity distribution based off of the cleaned tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(tokens_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `most_common(n)` provides a list of `n` length with sublist `[word, count]`\n",
    "### create a dataframe utilizing the 5 most common words in the cleaned token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mining</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>information</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cases</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>articles</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>learning</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>results</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>case</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>used</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0         text    255\n",
       "1         data    238\n",
       "2       mining    195\n",
       "3  information    153\n",
       "4        cases    150\n",
       "5     articles    124\n",
       "6     learning    111\n",
       "7      results    111\n",
       "8         case    110\n",
       "9         used    108"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fd.most_common()\n",
    "df_fd = pd.DataFrame(data[:n], columns = [\"word\", \"count\"])\n",
    "df_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a simple bar graph of top `n` words vs count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='word'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_fd.plot.bar(x='word', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# TO-DO\n",
    "### immediate\n",
    "* clustering method\n",
    " * tf-idf on each PDF then cull to look at score for top `n` words?\n",
    "\n",
    "### long-term\n",
    "* Look into API for digital commons\n",
    "* Adjust visualizations\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at `tagged` and `entities` from the top N words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = sorted(fd, key = fd.get, reverse = True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pos_tag` codes: https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('mining', 'NN'),\n",
       " ('information', 'NN'),\n",
       " ('cases', 'NNS'),\n",
       " ('articles', 'VBZ'),\n",
       " ('learning', 'VBG'),\n",
       " ('results', 'NNS'),\n",
       " ('case', 'NN'),\n",
       " ('used', 'VBN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(target_words)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [('text', 'NN'), ('data', 'NNS'), ('mining', 'NN'), ('information', 'NN'), ('cases', 'NNS'), ('articles', 'VBZ'), ('learning', 'VBG'), ('results', 'NNS'), ('case', 'NN'), ('used', 'VBN')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tagged), tagged, type(entities), entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## TF-IDF tests\n",
    "https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_words), len(tokens_removed), type(target_words), type(tokens_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = ''\n",
    "documentB = ''\n",
    "for word in target_words:\n",
    "    documentA += word+' '\n",
    "for word in tokens_removed:\n",
    "    documentB += word+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaai</th>\n",
       "      <th>aaamg</th>\n",
       "      <th>aams</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbreviations</th>\n",
       "      <th>abdel</th>\n",
       "      <th>abdurrahim</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬁgure</th>\n",
       "      <th>ﬁles</th>\n",
       "      <th>ﬁltered</th>\n",
       "      <th>ﬁltering</th>\n",
       "      <th>ﬁnancial</th>\n",
       "      <th>ﬁnd</th>\n",
       "      <th>ﬁnding</th>\n",
       "      <th>ﬁndings</th>\n",
       "      <th>ﬁnds</th>\n",
       "      <th>ﬁrst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00816</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.00816</td>\n",
       "      <td>0.00408</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4353 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa     aaai    aaamg     aams       ab  abbreviations    abdel  \\\n",
       "0  0.00000  0.00000  0.00000  0.00000  0.00000        0.00000  0.00000   \n",
       "1  0.00136  0.00816  0.00136  0.00136  0.00136        0.00136  0.00136   \n",
       "\n",
       "   abdurrahim  abilities  ability  ...    ﬁgure     ﬁles  ﬁltered  ﬁltering  \\\n",
       "0     0.00000    0.00000  0.00000  ...  0.00000  0.00000  0.00000   0.00000   \n",
       "1     0.00272    0.00136  0.00272  ...  0.00136  0.00272  0.00272   0.00272   \n",
       "\n",
       "   ﬁnancial     ﬁnd   ﬁnding  ﬁndings     ﬁnds     ﬁrst  \n",
       "0   0.00000  0.0000  0.00000  0.00000  0.00000  0.00000  \n",
       "1   0.00136  0.0068  0.00816  0.00408  0.00136  0.00408  \n",
       "\n",
       "[2 rows x 4353 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([documentA, documentB])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df2 = pd.DataFrame(denselist, columns=feature_names)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>case</th>\n",
       "      <th>cases</th>\n",
       "      <th>data</th>\n",
       "      <th>information</th>\n",
       "      <th>learning</th>\n",
       "      <th>mining</th>\n",
       "      <th>results</th>\n",
       "      <th>text</th>\n",
       "      <th>used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.119992</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>0.145152</td>\n",
       "      <td>0.230307</td>\n",
       "      <td>0.148055</td>\n",
       "      <td>0.107412</td>\n",
       "      <td>0.188697</td>\n",
       "      <td>0.107412</td>\n",
       "      <td>0.246758</td>\n",
       "      <td>0.104509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   articles      case     cases      data  information  learning    mining  \\\n",
       "0  0.316228  0.316228  0.316228  0.316228     0.316228  0.316228  0.316228   \n",
       "1  0.119992  0.106445  0.145152  0.230307     0.148055  0.107412  0.188697   \n",
       "\n",
       "    results      text      used  \n",
       "0  0.316228  0.316228  0.316228  \n",
       "1  0.107412  0.246758  0.104509  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set properties will allow us to remove every column that is not a targeted word from the early NLTK selection\n",
    "dropped_columns = list(set(feature_names).difference(target_words))\n",
    "df2.drop(columns = dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "Adapt example code to:\n",
    "* take unique tokens from each pdf being fed as input\n",
    "* store each token as a string in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>articles</th>\n",
       "      <th>case</th>\n",
       "      <th>cases</th>\n",
       "      <th>data</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>information</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>mining</th>\n",
       "      <th>one</th>\n",
       "      <th>results</th>\n",
       "      <th>second</th>\n",
       "      <th>text</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467561</td>\n",
       "      <td>0.563265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291075</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283390</td>\n",
       "      <td>0.503015</td>\n",
       "      <td>0.283390</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467561</td>\n",
       "      <td>0.563265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  articles      case     cases      data  document     first  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.467561  0.563265   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.692021  0.000000   \n",
       "2  0.503015  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.467561  0.563265   \n",
       "4  0.000000  0.316228  0.316228  0.316228  0.316228  0.000000  0.000000   \n",
       "\n",
       "   information        is  learning    mining       one   results    second  \\\n",
       "0     0.000000  0.393327  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.000000  0.291075  0.000000  0.000000  0.000000  0.000000  0.516656   \n",
       "2     0.000000  0.283390  0.000000  0.000000  0.503015  0.000000  0.000000   \n",
       "3     0.000000  0.393327  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.316228  0.000000  0.316228  0.316228  0.000000  0.316228  0.000000   \n",
       "\n",
       "       text       the     third      this      used  \n",
       "0  0.000000  0.393327  0.000000  0.393327  0.000000  \n",
       "1  0.000000  0.291075  0.000000  0.291075  0.000000  \n",
       "2  0.000000  0.283390  0.503015  0.283390  0.000000  \n",
       "3  0.000000  0.393327  0.000000  0.393327  0.000000  \n",
       "4  0.316228  0.000000  0.000000  0.000000  0.316228  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',\n",
    "          documentA]\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>case</th>\n",
       "      <th>cases</th>\n",
       "      <th>data</th>\n",
       "      <th>information</th>\n",
       "      <th>learning</th>\n",
       "      <th>mining</th>\n",
       "      <th>results</th>\n",
       "      <th>text</th>\n",
       "      <th>used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   articles      case     cases      data  information  learning    mining  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n",
       "4  0.316228  0.316228  0.316228  0.316228     0.316228  0.316228  0.316228   \n",
       "\n",
       "    results      text      used  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  \n",
       "4  0.316228  0.316228  0.316228  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set properties will allow us to remove every column that is not a targeted word from the early NLTK selection\n",
    "dropped_columns = list(set(feature_names).difference(target_words))\n",
    "df.drop(columns = dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_corpus.append(documentA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(multi_corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "dfM = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>case</th>\n",
       "      <th>cases</th>\n",
       "      <th>data</th>\n",
       "      <th>information</th>\n",
       "      <th>learning</th>\n",
       "      <th>mining</th>\n",
       "      <th>results</th>\n",
       "      <th>text</th>\n",
       "      <th>used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019338</td>\n",
       "      <td>0.070974</td>\n",
       "      <td>0.119250</td>\n",
       "      <td>0.042202</td>\n",
       "      <td>0.021704</td>\n",
       "      <td>0.031953</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.025924</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>0.031350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.048616</td>\n",
       "      <td>0.007366</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.017003</td>\n",
       "      <td>0.016205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029087</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091013</td>\n",
       "      <td>0.091013</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.084491</td>\n",
       "      <td>0.009893</td>\n",
       "      <td>0.134728</td>\n",
       "      <td>0.009893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121203</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.082537</td>\n",
       "      <td>0.068271</td>\n",
       "      <td>0.055025</td>\n",
       "      <td>0.159943</td>\n",
       "      <td>0.027512</td>\n",
       "      <td>0.206985</td>\n",
       "      <td>0.029550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.376980</td>\n",
       "      <td>0.325552</td>\n",
       "      <td>0.376980</td>\n",
       "      <td>0.282071</td>\n",
       "      <td>0.282071</td>\n",
       "      <td>0.282071</td>\n",
       "      <td>0.325552</td>\n",
       "      <td>0.282071</td>\n",
       "      <td>0.325552</td>\n",
       "      <td>0.282071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   articles      case     cases      data  information  learning    mining  \\\n",
       "0  0.019338  0.070974  0.119250  0.042202     0.021704  0.031953  0.002087   \n",
       "1  0.000000  0.006801  0.001969  0.048616     0.007366  0.001473  0.039107   \n",
       "2  0.029087  0.004567  0.000000  0.091013     0.091013  0.001979  0.084491   \n",
       "3  0.121203  0.004704  0.001362  0.082537     0.068271  0.055025  0.159943   \n",
       "4  0.000000  0.000000  0.000000  0.024611     0.003516  0.015821  0.000000   \n",
       "5  0.376980  0.325552  0.376980  0.282071     0.282071  0.282071  0.325552   \n",
       "\n",
       "    results      text      used  \n",
       "0  0.025924  0.015308  0.031350  \n",
       "1  0.014732  0.017003  0.016205  \n",
       "2  0.009893  0.134728  0.009893  \n",
       "3  0.027512  0.206985  0.029550  \n",
       "4  0.045706  0.000000  0.021095  \n",
       "5  0.282071  0.325552  0.282071  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set properties will allow us to remove every column that is not a targeted word from the early NLTK selection\n",
    "dropped_columns = list(set(feature_names).difference(target_words))\n",
    "dfM.drop(columns = dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MedvedevaEtAl2019.pdf\n",
      "1 KDD97-003.pdf\n",
      "2 P99-1001.pdf\n",
      "3 10.1007978-3-319-67056-018.pdf\n",
      "4 dummy_test.pdf\n"
     ]
    }
   ],
   "source": [
    "names = {}\n",
    "for x in range(len(files)):\n",
    "    print(str(x)+' '+str(files[x])[9:])\n",
    "    names[x] = str(files[x])[9:]\n",
    "names[x+1] = 'target_words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'MedvedevaEtAl2019.pdf',\n",
       " 1: 'KDD97-003.pdf',\n",
       " 2: 'P99-1001.pdf',\n",
       " 3: '10.1007978-3-319-67056-018.pdf',\n",
       " 4: 'dummy_test.pdf',\n",
       " 5: 'target_words'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>003</th>\n",
       "      <th>01</th>\n",
       "      <th>012</th>\n",
       "      <th>02</th>\n",
       "      <th>0250</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>050</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬁles</th>\n",
       "      <th>ﬁll</th>\n",
       "      <th>ﬁltered</th>\n",
       "      <th>ﬁltering</th>\n",
       "      <th>ﬁnancial</th>\n",
       "      <th>ﬁnd</th>\n",
       "      <th>ﬁnding</th>\n",
       "      <th>ﬁndings</th>\n",
       "      <th>ﬁnds</th>\n",
       "      <th>ﬁrst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedvedevaEtAl2019.pdf</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDD97-003.pdf</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P99-1001.pdf</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1007978-3-319-67056-018.pdf</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.013773</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.006887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_test.pdf</th>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_words</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 5361 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     00       000       003        01  \\\n",
       "MedvedevaEtAl2019.pdf           0.00000  0.000000  0.002716  0.001114   \n",
       "KDD97-003.pdf                   0.00000  0.000000  0.000000  0.000000   \n",
       "P99-1001.pdf                    0.00000  0.010965  0.000000  0.000000   \n",
       "10.1007978-3-319-67056-018.pdf  0.00000  0.001882  0.000000  0.000000   \n",
       "dummy_test.pdf                  0.00396  0.000000  0.000000  0.009742   \n",
       "target_words                    0.00000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                     012       02     0250       03       05  \\\n",
       "MedvedevaEtAl2019.pdf           0.001358  0.00000  0.00000  0.00000  0.00000   \n",
       "KDD97-003.pdf                   0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "P99-1001.pdf                    0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "10.1007978-3-319-67056-018.pdf  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "dummy_test.pdf                  0.000000  0.00396  0.00396  0.00396  0.00396   \n",
       "target_words                    0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "\n",
       "                                     050  ...      ﬁles       ﬁll   ﬁltered  \\\n",
       "MedvedevaEtAl2019.pdf           0.001358  ...  0.000000  0.000000  0.000000   \n",
       "KDD97-003.pdf                   0.000000  ...  0.000000  0.000000  0.000000   \n",
       "P99-1001.pdf                    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "10.1007978-3-319-67056-018.pdf  0.000000  ...  0.004591  0.002296  0.004591   \n",
       "dummy_test.pdf                  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "target_words                    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                ﬁltering  ﬁnancial       ﬁnd    ﬁnding  \\\n",
       "MedvedevaEtAl2019.pdf           0.000000  0.000000  0.000000  0.000000   \n",
       "KDD97-003.pdf                   0.000000  0.000000  0.000000  0.000000   \n",
       "P99-1001.pdf                    0.000000  0.000000  0.000000  0.000000   \n",
       "10.1007978-3-319-67056-018.pdf  0.004591  0.002296  0.011478  0.013773   \n",
       "dummy_test.pdf                  0.000000  0.000000  0.000000  0.000000   \n",
       "target_words                    0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                 ﬁndings      ﬁnds      ﬁrst  \n",
       "MedvedevaEtAl2019.pdf           0.000000  0.000000  0.000000  \n",
       "KDD97-003.pdf                   0.000000  0.000000  0.000000  \n",
       "P99-1001.pdf                    0.000000  0.000000  0.000000  \n",
       "10.1007978-3-319-67056-018.pdf  0.006887  0.002296  0.006887  \n",
       "dummy_test.pdf                  0.000000  0.000000  0.000000  \n",
       "target_words                    0.000000  0.000000  0.000000  \n",
       "\n",
       "[6 rows x 5361 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfM.rename(index=names, inplace = True)\n",
    "dfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>case</th>\n",
       "      <th>cases</th>\n",
       "      <th>data</th>\n",
       "      <th>information</th>\n",
       "      <th>learning</th>\n",
       "      <th>mining</th>\n",
       "      <th>results</th>\n",
       "      <th>text</th>\n",
       "      <th>used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedvedevaEtAl2019.pdf</th>\n",
       "      <td>0.019338</td>\n",
       "      <td>0.070974</td>\n",
       "      <td>0.119250</td>\n",
       "      <td>0.042202</td>\n",
       "      <td>0.021704</td>\n",
       "      <td>0.031953</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.025924</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>0.031350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDD97-003.pdf</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.048616</td>\n",
       "      <td>0.007366</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.017003</td>\n",
       "      <td>0.016205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P99-1001.pdf</th>\n",
       "      <td>0.029087</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091013</td>\n",
       "      <td>0.091013</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.084491</td>\n",
       "      <td>0.009893</td>\n",
       "      <td>0.134728</td>\n",
       "      <td>0.009893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.1007978-3-319-67056-018.pdf</th>\n",
       "      <td>0.121203</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.082537</td>\n",
       "      <td>0.068271</td>\n",
       "      <td>0.055025</td>\n",
       "      <td>0.159943</td>\n",
       "      <td>0.027512</td>\n",
       "      <td>0.206985</td>\n",
       "      <td>0.029550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_test.pdf</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_words</th>\n",
       "      <td>0.376980</td>\n",
       "      <td>0.325552</td>\n",
       "      <td>0.376980</td>\n",
       "      <td>0.282071</td>\n",
       "      <td>0.282071</td>\n",
       "      <td>0.282071</td>\n",
       "      <td>0.325552</td>\n",
       "      <td>0.282071</td>\n",
       "      <td>0.325552</td>\n",
       "      <td>0.282071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                articles      case     cases      data  \\\n",
       "MedvedevaEtAl2019.pdf           0.019338  0.070974  0.119250  0.042202   \n",
       "KDD97-003.pdf                   0.000000  0.006801  0.001969  0.048616   \n",
       "P99-1001.pdf                    0.029087  0.004567  0.000000  0.091013   \n",
       "10.1007978-3-319-67056-018.pdf  0.121203  0.004704  0.001362  0.082537   \n",
       "dummy_test.pdf                  0.000000  0.000000  0.000000  0.024611   \n",
       "target_words                    0.376980  0.325552  0.376980  0.282071   \n",
       "\n",
       "                                information  learning    mining   results  \\\n",
       "MedvedevaEtAl2019.pdf              0.021704  0.031953  0.002087  0.025924   \n",
       "KDD97-003.pdf                      0.007366  0.001473  0.039107  0.014732   \n",
       "P99-1001.pdf                       0.091013  0.001979  0.084491  0.009893   \n",
       "10.1007978-3-319-67056-018.pdf     0.068271  0.055025  0.159943  0.027512   \n",
       "dummy_test.pdf                     0.003516  0.015821  0.000000  0.045706   \n",
       "target_words                       0.282071  0.282071  0.325552  0.282071   \n",
       "\n",
       "                                    text      used  \n",
       "MedvedevaEtAl2019.pdf           0.015308  0.031350  \n",
       "KDD97-003.pdf                   0.017003  0.016205  \n",
       "P99-1001.pdf                    0.134728  0.009893  \n",
       "10.1007978-3-319-67056-018.pdf  0.206985  0.029550  \n",
       "dummy_test.pdf                  0.000000  0.021095  \n",
       "target_words                    0.325552  0.282071  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfM.drop(columns = dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
